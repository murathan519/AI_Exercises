# -*- coding: utf-8 -*-
"""PROJECT.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-5GXDdMVYCXlgqrt005czn86dDOctUiL

# **MURATHAN BAKIR 110180137**

## Importing the libraries
"""

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

"""## Importing the dataset

I prefer to read data from .csv file column by column and concatenate it again in wanted form. I eliminated 1. 4. 9. 11. and 12. columns as I mentioned in the report.
"""

train_set = pd.read_csv('train.csv')
test_set = pd.read_csv('test.csv')
#Train Set
y_train = train_set.iloc[:, 1].values       #2nd column Survived
x_train_Pclass = train_set.iloc[:,2].values #3rd column Pclass
x_train_Sex = train_set.iloc[:,4].values    #5th column Sex
x_train_Age = train_set.iloc[:,5].values    #6th column Age
x_train_SibSp = train_set.iloc[:,6].values  #7th column SibSp
x_train_Parch = train_set.iloc[:,7].values  #8th column Parch
x_train_Fare = train_set.iloc[:,9].values   #10th column Fare
#Test Set
x_test_Pclass = test_set.iloc[:,1].values
x_test_Sex = test_set.iloc[:,3].values
x_test_Age = test_set.iloc[:,4].values
x_test_SibSp = test_set.iloc[:,5].values
x_test_Parch = test_set.iloc[:,6].values
x_test_Fare = test_set.iloc[:,8].values

"""# Concatenating Whole Data

I reshaped the taken columns by making them vertical and concatenated.
"""

x_train_new = np.concatenate((x_train_Pclass.reshape(len(x_train_Pclass),1), x_train_Sex.reshape(len(x_train_Sex),1), x_train_Age.reshape(len(x_train_Age),1), x_train_SibSp.reshape(len(x_train_SibSp),1), x_train_Parch.reshape(len(x_train_Parch),1), x_train_Fare.reshape(len(x_train_Fare),1)),1)
x_test_new = np.concatenate((x_test_Pclass.reshape(len(x_test_Pclass),1), x_test_Sex.reshape(len(x_test_Sex),1), x_test_Age.reshape(len(x_test_Age),1), x_test_SibSp.reshape(len(x_test_SibSp),1), x_test_Parch.reshape(len(x_test_Parch),1), x_test_Fare.reshape(len(x_test_Fare),1)),1)

print("Train Set")
print(x_train_new)
print("\nTest Set")
print(x_test_new)

"""# Taking Care of Missing Data

Approximately 20% of the age data is missing. I fill the gaps by putting the average value of the age column (29.7 years).
"""

from sklearn.impute import SimpleImputer
#Train Set
imputer_train = SimpleImputer(missing_values=np.nan, strategy='mean')
imputer_train.fit(x_train_new[:,2:])  #all the rows and from 2nd column until the last column filled with mean value 
x_train_new[:,2:] = imputer_train.transform(x_train_new[:,2:])
#Test Set
imputer_test = SimpleImputer(missing_values=np.nan, strategy='mean')
imputer_test.fit(x_test_new[:,2:])    #all the rows and from 2nd column until the last column filled with mean value 
x_test_new[:,2:] = imputer_test.transform(x_test_new[:,2:])

print("Train Set")
print(x_train_new)
print("\nTest Set")
print(x_test_new)

"""# Encoding Categorical Data

Sex is a categorical data, we should convert it to numerical data in order to make the feature scaling succesfully.
"""

from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder

ct_train = ColumnTransformer(transformers=[('encoder',OneHotEncoder(),[1])], remainder='passthrough')
x_train_new = np.array(ct_train.fit_transform(x_train_new))

ct_test = ColumnTransformer(transformers=[('encoder',OneHotEncoder(),[1])], remainder='passthrough')
x_test_new = np.array(ct_train.fit_transform(x_test_new))

"""# **PLOTS**

Before future scaling, let's take a look at statistics.

#Sex

The strongest correlation is between gender and survival ratios as seen in the graphs. Men die much more.

*   Left graph shows the gender ratio of death people.
*   Right graph shows the gender ratio of people alive.




*   Vertical axis shows number of people.
*   Horizontal axis shows genders (left men, right women).
"""

data_plot_train = pd.read_csv('train.csv',usecols=['Age','Fare','Sex','Pclass','SibSp','Parch','Survived']) #creating a subset for just plots 
data_plot_train.hist(column='Sex', by='Survived', bins=2, grid=False, rwidth=.5, color='pink', sharex=True)

"""#Age 
It is a little bit dissapointing, because there is no strong correlations between age and survival ratio as I expected. However we can still say that younger people are more advantageous rather than older people.

*   In horizontal axis, 0 represents death people and 1 represents people alive. 
*   In vertical axis, numbers show ages of people. 


It is hard to see the weak correlation in scatter plot. Let's try it with bar plot.
"""

plt.scatter(y_train,x_train_Age,0.05)

"""*   Left graph shows death people's ages.
*   Right graph shows alive people's ages.




*   Vertical axis shows number of people 
*   Horizontal axis shows people's age.
"""

data_plot_train.hist(column='Age', by='Survived', bins=20, grid=False, rwidth=.5, color='pink', sharex=True)

"""#Fare
There is also a weak correlation between fare and surivive rates. Survival change increasing with increasing fare.

*   In horizontal axis, 0 represents death people and 1 represents people alive.
*   In vertical axis, numbers show fare of the ticket.
"""

plt.scatter(y_train,x_train_Fare,0.03)

"""*   Left graph shows the ticket fare of death people.
*   Right graph shows the ticket fare of people alive.




*   Vertical axis shows number of people.
*   Horizontal axis shows ticket fare.
"""

data_plot_train.hist(column='Fare', by='Survived', bins=10, grid=False, rwidth=.5, color='pink', sharex=True)

"""#Ticket Class

As seen in the graphs first class passengers are tend to survive rather than lower classes.

*   Left graph shows the ticket class of death people.
*   Right graph shows the ticket class of people alive.



*   Vertical axis shows number of people.
*   Horizontal axis shows ticket class.
"""

data_plot_train.hist(column='Pclass', by='Survived', bins=3, grid=False, rwidth=0.5, color='pink', sharex=True)

"""# Sibling & Spouse Number

We can see that probablity of surviving is higher at people with siblings or spouses.

*   Left graph shows the companion numbers of death people.
*   Right graph shows the companion numbers of people alive.



*   Vertical axis shows number of people.
*   Horizontal axis shows companion numbers of individuals.
"""

data_plot_train.hist(column='SibSp', by='Survived', bins=10, grid=False, rwidth=.5, color='pink', sharex=True)

"""# Parent & Children Number

We can see that probablity of surviving is higher at people with parents or children. I assume that during the recovery operations, women with children got a big priority according to gender and parch results.

*   Left graph shows the companion numbers of death people.
*   Right graph shows the companion numbers of people alive.



*   Vertical axis shows number of people.
*   Horizontal axis shows parental companion numbers of individuals.
"""

data_plot_train.hist(column='Parch', by='Survived', bins=5, grid=False, rwidth=.5, color='pink', sharex=True)

"""## Feature Scaling"""

from sklearn.preprocessing import StandardScaler
sc = StandardScaler()

x_train_new = sc.fit_transform(x_train_new)
x_test_new = sc.transform(x_test_new)

"""# **MODELS**

## Training the Decision Tree Classification Model on the Training Set
"""

from sklearn.tree import DecisionTreeClassifier
classifier = DecisionTreeClassifier(criterion = 'entropy')
classifier.fit(x_train_new, y_train)

"""#Predicting the Test Set Results (Decision Tree)"""

y_pred = classifier.predict(x_test_new)
print(y_pred)

"""# Training the Kernel SVM Model on the Training Set"""

from sklearn.svm import SVC
classifier = SVC(kernel = 'rbf')
classifier.fit(x_train_new, y_train)

"""# Predicting the Test Set Results (Kernel SVM)"""

y_pred = classifier.predict(x_test_new)
print(y_pred)

"""# Training the Random Forest Classification Model on the Training Set"""

from sklearn.ensemble import RandomForestClassifier
classifier = RandomForestClassifier(n_estimators = 100, criterion = 'entropy')
classifier.fit(x_train_new, y_train)

"""# Predicting the Test Set Results (Random Forest)"""

y_pred = classifier.predict(x_test_new)
print(y_pred)

"""# Training the Naive Bayes Model on the Training Set"""

from sklearn.naive_bayes import GaussianNB
classifier = GaussianNB()
classifier.fit(x_train_new, y_train)

"""# Predicting the Test Set Results (Naive Bayes)"""

y_pred = classifier.predict(x_test_new)
print(y_pred)

"""# Training the K-Nearest Model on the Training Set"""

from sklearn.neighbors import KNeighborsClassifier
classifier = KNeighborsClassifier(n_neighbors = 20, metric = 'minkowski', p = 2)
classifier.fit(x_train_new, y_train)

"""# Predicting the Test Set Results (K-Nearest)"""

y_pred = classifier.predict(x_test_new)
print(y_pred)

"""# Training the Logistic Regression Model on the Training Set"""

from sklearn.linear_model import LogisticRegression
classifier = LogisticRegression()
classifier.fit(x_train_new, y_train)

"""# Predicting the Test Set Results (Logistic Regression)"""

y_pred = classifier.predict(x_test_new)
print(y_pred)

"""# Training the Support Vector Machine  Model on the Training Set"""

from sklearn.svm import SVC
classifier = SVC(kernel = 'poly')
classifier.fit(x_train_new, y_train)

"""# Predicting the Test Set Results (SVM)"""

y_pred = classifier.predict(x_test_new)
print(y_pred)